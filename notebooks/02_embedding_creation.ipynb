{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 2: Text Chunking, Embedding, and Vector Store Indexing\n",
        "\n",
        "## Objective\n",
        "To convert the cleaned text narratives into a format suitable for efficient semantic search.\n",
        "\n",
        "## Steps:\n",
        "1. Implement text chunking strategy\n",
        "2. Choose and justify embedding model\n",
        "3. Generate embeddings for text chunks\n",
        "4. Create vector store using ChromaDB\n",
        "5. Store embeddings with metadata\n",
        "6. Test retrieval functionality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: onnxruntime in c:\\users\\doff n\\desktop\\vector\\week 6\\venv\\lib\\site-packages (1.22.0)"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.2.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Requirement already satisfied: packaging in c:\\users\\doff n\\desktop\\vector\\week 6\\venv\\lib\\site-packages (from onnxruntime) (23.2)\n",
            "Requirement already satisfied: sympy in c:\\users\\doff n\\desktop\\vector\\week 6\\venv\\lib\\site-packages (from onnxruntime) (1.14.0)\n",
            "Requirement already satisfied: protobuf in c:\\users\\doff n\\desktop\\vector\\week 6\\venv\\lib\\site-packages (from onnxruntime) (5.29.5)\n",
            "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\doff n\\desktop\\vector\\week 6\\venv\\lib\\site-packages (from onnxruntime) (1.24.3)\n",
            "Requirement already satisfied: flatbuffers in c:\\users\\doff n\\desktop\\vector\\week 6\\venv\\lib\\site-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: coloredlogs in c:\\users\\doff n\\desktop\\vector\\week 6\\venv\\lib\\site-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\doff n\\desktop\\vector\\week 6\\venv\\lib\\site-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\doff n\\desktop\\vector\\week 6\\venv\\lib\\site-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: pyreadline3 in c:\\users\\doff n\\desktop\\vector\\week 6\\venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime) (3.5.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install onnxruntime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "The onnxruntime python package is not installed. Please install it with `pip install onnxruntime`",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\doff n\\Desktop\\vector\\week 6\\venv\\lib\\site-packages\\chromadb\\utils\\embedding_functions.py:269\u001b[0m, in \u001b[0;36mONNXMiniLM_L6_V2.__init__\u001b[1;34m(self, preferred_providers)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;66;03m# Equivalent to import onnxruntime\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mort \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43monnxruntime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\doff n\\Desktop\\vector\\week 6\\venv\\lib\\site-packages\\onnxruntime\\__init__.py:61\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m import_capi_exception:\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m import_capi_exception\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01monnxruntime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnxruntime_inference_collection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     64\u001b[0m     AdapterFormat,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     InferenceSession,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     69\u001b[0m     SparseTensor,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     70\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\doff n\\Desktop\\vector\\week 6\\venv\\lib\\site-packages\\onnxruntime\\__init__.py:24\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01monnxruntime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pybind_state\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     25\u001b[0m         ExecutionMode,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     26\u001b[0m         ExecutionOrder,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     27\u001b[0m         GraphOptimizationLevel,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     28\u001b[0m         LoraAdapter,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     29\u001b[0m         ModelMetadata,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     30\u001b[0m         NodeArg,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     31\u001b[0m         OrtAllocatorType,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     32\u001b[0m         OrtArenaCfg,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     33\u001b[0m         OrtMemoryInfo,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     34\u001b[0m         OrtMemType,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     35\u001b[0m         OrtSparseFormat,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         RunOptions,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     37\u001b[0m         SessionIOBinding,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     38\u001b[0m         SessionOptions,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     39\u001b[0m         create_and_register_allocator,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     40\u001b[0m         create_and_register_allocator_v2,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     41\u001b[0m         disable_telemetry_events,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     42\u001b[0m         enable_telemetry_events,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     43\u001b[0m         get_all_providers,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     44\u001b[0m         get_available_providers,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     45\u001b[0m         get_build_info,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     46\u001b[0m         get_device,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     47\u001b[0m         get_version_string,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     48\u001b[0m         has_collective_ops,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     49\u001b[0m         set_default_logger_severity,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     50\u001b[0m         set_default_logger_verbosity,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     51\u001b[0m         set_seed,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     )\n\u001b[0;32m     54\u001b[0m     import_capi_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\doff n\\Desktop\\vector\\week 6\\venv\\lib\\site-packages\\onnxruntime\\capi\\_pybind_state.py:32\u001b[0m\n\u001b[0;32m     25\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install the 2019 Visual C++ runtime and then try again. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve installed the runtime in a non-standard location \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(other than \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mSystemRoot\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mSystem32), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake sure it can be found by setting the correct path.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnxruntime_pybind11_state\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
            "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing onnxruntime_pybind11_state: A dynamic link library (DLL) initialization routine failed.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Settings\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01muuid\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\doff n\\Desktop\\vector\\week 6\\venv\\lib\\site-packages\\chromadb\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Client \u001b[38;5;28;01mas\u001b[39;00m ClientCreator\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AdminClient \u001b[38;5;28;01mas\u001b[39;00m AdminClientCreator\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\doff n\\Desktop\\vector\\week 6\\venv\\lib\\site-packages\\chromadb\\api\\__init__.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moverrides\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m override\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_DATABASE, DEFAULT_TENANT\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCollection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Collection\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     CollectionMetadata,\n\u001b[0;32m     10\u001b[0m     Documents,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     WhereDocument,\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Component, Settings\n",
            "File \u001b[1;32mc:\\Users\\doff n\\Desktop\\vector\\week 6\\venv\\lib\\site-packages\\chromadb\\api\\models\\Collection.py:39\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ServerAPI\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCollection\u001b[39;00m(BaseModel):\n\u001b[0;32m     40\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mid\u001b[39m: UUID\n",
            "File \u001b[1;32mc:\\Users\\doff n\\Desktop\\vector\\week 6\\venv\\lib\\site-packages\\chromadb\\api\\models\\Collection.py:51\u001b[0m, in \u001b[0;36mCollection\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m _client: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServerAPI\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m PrivateAttr()\n\u001b[0;32m     44\u001b[0m _embedding_function: Optional[EmbeddingFunction] \u001b[38;5;241m=\u001b[39m PrivateAttr()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     48\u001b[0m     client: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServerAPI\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     49\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mid\u001b[39m: UUID,\n\u001b[1;32m---> 51\u001b[0m     embedding_function: Optional[EmbeddingFunction] \u001b[38;5;241m=\u001b[39m \u001b[43mef\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDefaultEmbeddingFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     52\u001b[0m     metadata: Optional[CollectionMetadata] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     53\u001b[0m ):\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name\u001b[38;5;241m=\u001b[39mname, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m)\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m client\n",
            "File \u001b[1;32mc:\\Users\\doff n\\Desktop\\vector\\week 6\\venv\\lib\\site-packages\\chromadb\\utils\\embedding_functions.py:420\u001b[0m, in \u001b[0;36mDefaultEmbeddingFunction\u001b[1;34m()\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mONNXMiniLM_L6_V2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\doff n\\Desktop\\vector\\week 6\\venv\\lib\\site-packages\\chromadb\\utils\\embedding_functions.py:271\u001b[0m, in \u001b[0;36mONNXMiniLM_L6_V2.__init__\u001b[1;34m(self, preferred_providers)\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mort \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnxruntime\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m--> 271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    272\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe onnxruntime python package is not installed. Please install it with `pip install onnxruntime`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m     )\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;66;03m# Equivalent to from tokenizers import Tokenizer\u001b[39;00m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTokenizer \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mTokenizer\n",
            "\u001b[1;31mValueError\u001b[0m: The onnxruntime python package is not installed. Please install it with `pip install onnxruntime`"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import uuid\n",
        "from typing import List, Dict, Any\n",
        "import json\n",
        "import os\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Processed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the processed complaint data from Task 1\n",
        "data_path = '../data/filtered_complaints.csv'\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "    print(\"‚ùå Processed data not found. Please run Task 1 first.\")\n",
        "    print(\"Expected file: ../data/filtered_complaints.csv\")\n",
        "    raise FileNotFoundError(\"Run notebook 01_data_exploration.ipynb first\")\n",
        "\n",
        "print(\"Loading processed complaint data...\")\n",
        "df = pd.read_csv(data_path)\n",
        "print(f\"‚úÖ Loaded {len(df):,} complaint records\")\n",
        "print(f\"üìä Columns: {list(df.columns)}\")\n",
        "\n",
        "# Display basic statistics\n",
        "print(f\"\\nüìà Dataset Statistics:\")\n",
        "print(f\"Products: {df['Product'].nunique()} unique ({', '.join(df['Product'].unique())})\")\n",
        "print(f\"Average narrative length: {df['cleaned_word_count'].mean():.1f} words\")\n",
        "print(f\"Median narrative length: {df['cleaned_word_count'].median():.1f} words\")\n",
        "print(f\"Date range: {df['Date received'].min()} to {df['Date received'].max()}\")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Text Chunking Strategy\n",
        "\n",
        "### Why Chunking?\n",
        "Long narratives are often ineffective when embedded as a single vector because:\n",
        "- They may contain multiple distinct topics\n",
        "- Embedding models have token limits\n",
        "- Smaller chunks provide more precise retrieval\n",
        "\n",
        "### Chunking Parameters:\n",
        "- **Chunk Size**: 500 characters (balance between context and precision)\n",
        "- **Overlap**: 50 characters (maintain context continuity)\n",
        "- **Strategy**: Recursive character splitting (respects sentence boundaries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze narrative lengths to determine optimal chunking strategy\n",
        "print(\"=== NARRATIVE LENGTH ANALYSIS ===\")\n",
        "\n",
        "# Character count analysis\n",
        "char_counts = df['cleaned_narrative'].str.len()\n",
        "print(f\"Character count statistics:\")\n",
        "print(char_counts.describe())\n",
        "\n",
        "# Visualize length distribution\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Histogram of character counts\n",
        "ax1.hist(char_counts, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "ax1.set_title('Distribution of Narrative Character Counts', fontweight='bold')\n",
        "ax1.set_xlabel('Character Count')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.axvline(char_counts.mean(), color='red', linestyle='--', label=f'Mean: {char_counts.mean():.0f}')\n",
        "ax1.axvline(char_counts.median(), color='green', linestyle='--', label=f'Median: {char_counts.median():.0f}')\n",
        "ax1.legend()\n",
        "\n",
        "# Box plot by product\n",
        "df.boxplot(column='cleaned_word_count', by='Product', ax=ax2)\n",
        "ax2.set_title('Narrative Length by Product', fontweight='bold')\n",
        "ax2.set_xlabel('Product')\n",
        "ax2.set_ylabel('Word Count')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Determine chunking parameters\n",
        "chunk_size = 500  # characters\n",
        "chunk_overlap = 50  # characters\n",
        "\n",
        "print(f\"\\n=== CHUNKING STRATEGY ===\")\n",
        "print(f\"Chunk size: {chunk_size} characters\")\n",
        "print(f\"Chunk overlap: {chunk_overlap} characters\")\n",
        "print(f\"Rationale:\")\n",
        "print(f\"  - Median narrative length: {char_counts.median():.0f} chars\")\n",
        "print(f\"  - {chunk_size} chars ‚âà 75-100 words (good for semantic coherence)\")\n",
        "print(f\"  - {chunk_overlap} chars overlap maintains context continuity\")\n",
        "print(f\"  - Recursive splitting respects sentence boundaries\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement text chunking\n",
        "def create_text_chunks(df: pd.DataFrame, chunk_size: int = 500, chunk_overlap: int = 50) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Split complaint narratives into chunks for better embedding\"\"\"\n",
        "    print(\"Creating text chunks...\")\n",
        "    \n",
        "    # Initialize text splitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"]\n",
        "    )\n",
        "    \n",
        "    chunks = []\n",
        "    \n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing narratives\"):\n",
        "        narrative = row['cleaned_narrative']\n",
        "        \n",
        "        # Split text into chunks\n",
        "        text_chunks = text_splitter.split_text(narrative)\n",
        "        \n",
        "        for chunk_idx, chunk in enumerate(text_chunks):\n",
        "            if len(chunk.strip()) > 20:  # Only keep meaningful chunks\n",
        "                chunk_data = {\n",
        "                    'id': f\"{idx}_{chunk_idx}\",\n",
        "                    'text': chunk.strip(),\n",
        "                    'complaint_id': idx,\n",
        "                    'product': row['Product'],\n",
        "                    'issue': row.get('Issue', 'Unknown'),\n",
        "                    'company': row.get('Company', 'Unknown'),\n",
        "                    'date_received': str(row.get('Date received', 'Unknown')),\n",
        "                    'state': row.get('State', 'Unknown'),\n",
        "                    'chunk_index': chunk_idx,\n",
        "                    'original_length': len(narrative),\n",
        "                    'chunk_length': len(chunk),\n",
        "                    'total_chunks': len(text_chunks)\n",
        "                }\n",
        "                chunks.append(chunk_data)\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "# Create chunks\n",
        "chunks = create_text_chunks(df, chunk_size, chunk_overlap)\n",
        "\n",
        "print(f\"\\n‚úÖ Created {len(chunks):,} text chunks from {len(df):,} complaints\")\n",
        "print(f\"üìä Average chunks per complaint: {len(chunks)/len(df):.1f}\")\n",
        "print(f\"üìè Average chunk length: {np.mean([c['chunk_length'] for c in chunks]):.0f} characters\")\n",
        "\n",
        "# Analyze chunking results\n",
        "chunk_stats = pd.DataFrame(chunks)\n",
        "print(f\"\\n=== CHUNKING STATISTICS ===\")\n",
        "print(f\"Chunk length distribution:\")\n",
        "print(chunk_stats['chunk_length'].describe())\n",
        "\n",
        "print(f\"\\nChunks per product:\")\n",
        "chunks_per_product = chunk_stats['product'].value_counts()\n",
        "print(chunks_per_product)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize chunking results\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Chunk length distribution\n",
        "chunk_lengths = [c['chunk_length'] for c in chunks]\n",
        "ax1.hist(chunk_lengths, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "ax1.set_title('Distribution of Chunk Lengths', fontweight='bold')\n",
        "ax1.set_xlabel('Chunk Length (characters)')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.axvline(np.mean(chunk_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(chunk_lengths):.0f}')\n",
        "ax1.legend()\n",
        "\n",
        "# Chunks per complaint distribution\n",
        "chunks_per_complaint = chunk_stats.groupby('complaint_id')['chunk_index'].count()\n",
        "ax2.hist(chunks_per_complaint, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "ax2.set_title('Distribution of Chunks per Complaint', fontweight='bold')\n",
        "ax2.set_xlabel('Number of Chunks')\n",
        "ax2.set_ylabel('Frequency')\n",
        "ax2.axvline(chunks_per_complaint.mean(), color='red', linestyle='--', label=f'Mean: {chunks_per_complaint.mean():.1f}')\n",
        "ax2.legend()\n",
        "\n",
        "# Chunks by product\n",
        "chunks_per_product.plot(kind='bar', ax=ax3, color='gold')\n",
        "ax3.set_title('Number of Chunks by Product', fontweight='bold')\n",
        "ax3.set_xlabel('Product')\n",
        "ax3.set_ylabel('Number of Chunks')\n",
        "ax3.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Original vs chunk length relationship\n",
        "original_lengths = [c['original_length'] for c in chunks]\n",
        "ax4.scatter(original_lengths, chunk_lengths, alpha=0.5, color='purple')\n",
        "ax4.set_title('Original vs Chunk Length', fontweight='bold')\n",
        "ax4.set_xlabel('Original Narrative Length')\n",
        "ax4.set_ylabel('Chunk Length')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show example chunks\n",
        "print(\"\\n=== EXAMPLE CHUNKS ===\")\n",
        "for i in range(min(3, len(chunks))):\n",
        "    chunk = chunks[i]\n",
        "    print(f\"\\nChunk {i+1} (Product: {chunk['product']}, Issue: {chunk['issue']}):\")\n",
        "    print(f\"Length: {chunk['chunk_length']} chars\")\n",
        "    print(f\"Text: {chunk['text'][:200]}{'...' if len(chunk['text']) > 200 else ''}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Embedding Model Selection\n",
        "\n",
        "### Model Choice: `sentence-transformers/all-MiniLM-L6-v2`\n",
        "\n",
        "**Rationale:**\n",
        "- **Performance**: Good balance of quality and speed\n",
        "- **Size**: Lightweight (80MB) for efficient deployment\n",
        "- **Domain**: Trained on diverse text, suitable for financial complaints\n",
        "- **Dimensions**: 384-dimensional embeddings (manageable size)\n",
        "- **Language**: Optimized for English text\n",
        "- **Popularity**: Well-tested and widely used in production"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize embedding model\n",
        "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "print(f\"=== EMBEDDING MODEL INITIALIZATION ===\")\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Loading model...\")\n",
        "\n",
        "start_time = time.time()\n",
        "embedding_model = SentenceTransformer(model_name)\n",
        "load_time = time.time() - start_time\n",
        "\n",
        "print(f\"‚úÖ Model loaded in {load_time:.2f} seconds\")\n",
        "\n",
        "# Get model information\n",
        "print(f\"\\nüìã Model Information:\")\n",
        "print(f\"Model name: {model_name}\")\n",
        "print(f\"Max sequence length: {embedding_model.max_seq_length}\")\n",
        "print(f\"Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n",
        "\n",
        "# Test embedding generation\n",
        "test_text = \"This is a test complaint about billing issues with my credit card.\"\n",
        "test_embedding = embedding_model.encode([test_text])\n",
        "print(f\"\\nüß™ Test embedding:\")\n",
        "print(f\"Input text: {test_text}\")\n",
        "print(f\"Embedding shape: {test_embedding.shape}\")\n",
        "print(f\"Embedding sample: {test_embedding[0][:5]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Generate Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate embeddings for all chunks\n",
        "def create_embeddings(chunks: List[Dict[str, Any]], model: SentenceTransformer, batch_size: int = 32) -> np.ndarray:\n",
        "    \"\"\"Generate embeddings for text chunks\"\"\"\n",
        "    print(\"Generating embeddings...\")\n",
        "    \n",
        "    texts = [chunk['text'] for chunk in chunks]\n",
        "    \n",
        "    try:\n",
        "        # Generate embeddings in batches to manage memory\n",
        "        embeddings = []\n",
        "        \n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "            batch_embeddings = model.encode(\n",
        "                batch_texts, \n",
        "                show_progress_bar=False,\n",
        "                convert_to_numpy=True,\n",
        "                normalize_embeddings=True  # Normalize for cosine similarity\n",
        "            )\n",
        "            embeddings.append(batch_embeddings)\n",
        "        \n",
        "        embeddings = np.vstack(embeddings)\n",
        "        return embeddings\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating embeddings: {e}\")\n",
        "        raise\n",
        "\n",
        "# Generate embeddings\n",
        "start_time = time.time()\n",
        "embeddings = create_embeddings(chunks, embedding_model, batch_size=32)\n",
        "embedding_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n‚úÖ Generated {len(embeddings):,} embeddings\")\n",
        "print(f\"üìä Embedding shape: {embeddings.shape}\")\n",
        "print(f\"‚è±Ô∏è  Generation time: {embedding_time:.2f} seconds\")\n",
        "print(f\"üöÄ Speed: {len(embeddings)/embedding_time:.1f} embeddings/second\")\n",
        "print(f\"üíæ Memory usage: {embeddings.nbytes / 1024**2:.2f} MB\")\n",
        "\n",
        "# Analyze embedding statistics\n",
        "print(f\"\\n=== EMBEDDING STATISTICS ===\")\n",
        "print(f\"Mean embedding norm: {np.linalg.norm(embeddings, axis=1).mean():.4f}\")\n",
        "print(f\"Std embedding norm: {np.linalg.norm(embeddings, axis=1).std():.4f}\")\n",
        "print(f\"Min embedding value: {embeddings.min():.4f}\")\n",
        "print(f\"Max embedding value: {embeddings.max():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize embedding properties\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Embedding norms distribution\n",
        "norms = np.linalg.norm(embeddings, axis=1)\n",
        "ax1.hist(norms, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "ax1.set_title('Distribution of Embedding Norms', fontweight='bold')\n",
        "ax1.set_xlabel('L2 Norm')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.axvline(norms.mean(), color='red', linestyle='--', label=f'Mean: {norms.mean():.3f}')\n",
        "ax1.legend()\n",
        "\n",
        "# Embedding values distribution\n",
        "ax2.hist(embeddings.flatten(), bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "ax2.set_title('Distribution of Embedding Values', fontweight='bold')\n",
        "ax2.set_xlabel('Embedding Value')\n",
        "ax2.set_ylabel('Frequency')\n",
        "\n",
        "# PCA visualization (first 2 components)\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Sample embeddings for visualization (to avoid overcrowding)\n",
        "sample_size = min(1000, len(embeddings))\n",
        "sample_indices = np.random.choice(len(embeddings), sample_size, replace=False)\n",
        "sample_embeddings = embeddings[sample_indices]\n",
        "sample_chunks = [chunks[i] for i in sample_indices]\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "embeddings_2d = pca.fit_transform(sample_embeddings)\n",
        "\n",
        "# Color by product\n",
        "products = [chunk['product'] for chunk in sample_chunks]\n",
        "unique_products = list(set(products))\n",
        "colors = plt.cm.Set3(np.linspace(0, 1, len(unique_products)))\n",
        "product_colors = {product: colors[i] for i, product in enumerate(unique_products)}\n",
        "\n",
        "for product in unique_products:\n",
        "    mask = [p == product for p in products]\n",
        "    ax3.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], \n",
        "               c=[product_colors[product]], label=product, alpha=0.6, s=20)\n",
        "\n",
        "ax3.set_title('PCA Visualization of Embeddings (by Product)', fontweight='bold')\n",
        "ax3.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
        "ax3.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
        "ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Similarity heatmap (sample)\n",
        "sample_similarity = np.dot(sample_embeddings[:20], sample_embeddings[:20].T)\n",
        "im = ax4.imshow(sample_similarity, cmap='viridis', aspect='auto')\n",
        "ax4.set_title('Similarity Matrix (Sample)', fontweight='bold')\n",
        "ax4.set_xlabel('Chunk Index')\n",
        "ax4.set_ylabel('Chunk Index')\n",
        "plt.colorbar(im, ax=ax4)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìä PCA Analysis:\")\n",
        "print(f\"PC1 explains {pca.explained_variance_ratio_[0]:.1%} of variance\")\n",
        "print(f\"PC2 explains {pca.explained_variance_ratio_[1]:.1%} of variance\")\n",
        "print(f\"Total variance explained: {pca.explained_variance_ratio_.sum():.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Create Vector Store with ChromaDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize ChromaDB\n",
        "vector_store_path = '../vector_store'\n",
        "\n",
        "print(f\"=== VECTOR STORE CREATION ===\")\n",
        "print(f\"Vector store path: {vector_store_path}\")\n",
        "\n",
        "# Create vector store directory\n",
        "os.makedirs(vector_store_path, exist_ok=True)\n",
        "\n",
        "# Initialize ChromaDB client\n",
        "chroma_client = chromadb.PersistentClient(path=vector_store_path)\n",
        "print(f\"‚úÖ ChromaDB client initialized\")\n",
        "\n",
        "# Create or get collection\n",
        "collection_name = \"complaint_embeddings\"\n",
        "\n",
        "# Delete existing collection if it exists\n",
        "try:\n",
        "    chroma_client.delete_collection(name=collection_name)\n",
        "    print(f\"üóëÔ∏è  Deleted existing collection: {collection_name}\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Create new collection\n",
        "collection = chroma_client.create_collection(\n",
        "    name=collection_name,\n",
        "    metadata={\"description\": \"Financial complaint embeddings for RAG system\"}\n",
        ")\n",
        "print(f\"‚úÖ Created collection: {collection_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for ChromaDB\n",
        "def prepare_chroma_data(chunks: List[Dict[str, Any]], embeddings: np.ndarray):\n",
        "    \"\"\"Prepare data for ChromaDB insertion\"\"\"\n",
        "    \n",
        "    ids = [chunk['id'] for chunk in chunks]\n",
        "    documents = [chunk['text'] for chunk in chunks]\n",
        "    metadatas = []\n",
        "    \n",
        "    for chunk in chunks:\n",
        "        metadata = {\n",
        "            'complaint_id': str(chunk['complaint_id']),\n",
        "            'product': chunk['product'],\n",
        "            'issue': chunk['issue'],\n",
        "            'company': chunk['company'],\n",
        "            'date_received': chunk['date_received'],\n",
        "            'state': chunk['state'],\n",
        "            'chunk_index': chunk['chunk_index'],\n",
        "            'original_length': chunk['original_length'],\n",
        "            'chunk_length': chunk['chunk_length'],\n",
        "            'total_chunks': chunk['total_chunks']\n",
        "        }\n",
        "        metadatas.append(metadata)\n",
        "    \n",
        "    return ids, documents, metadatas, embeddings.tolist()\n",
        "\n",
        "# Prepare data\n",
        "print(\"Preparing data for ChromaDB...\")\n",
        "ids, documents, metadatas, embedding_list = prepare_chroma_data(chunks, embeddings)\n",
        "\n",
        "print(f\"‚úÖ Prepared {len(ids):,} records for insertion\")\n",
        "print(f\"üìä Sample metadata: {metadatas[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Insert data into ChromaDB in batches\n",
        "def insert_to_chroma(collection, ids, documents, metadatas, embeddings, batch_size=100):\n",
        "    \"\"\"Insert data into ChromaDB collection in batches\"\"\"\n",
        "    \n",
        "    total_batches = (len(ids) + batch_size - 1) // batch_size\n",
        "    \n",
        "    for i in tqdm(range(0, len(ids), batch_size), desc=\"Inserting to ChromaDB\"):\n",
        "        end_idx = min(i + batch_size, len(ids))\n",
        "        \n",
        "        batch_ids = ids[i:end_idx]\n",
        "        batch_documents = documents[i:end_idx]\n",
        "        batch_metadatas = metadatas[i:end_idx]\n",
        "        batch_embeddings = embeddings[i:end_idx]\n",
        "        \n",
        "        collection.add(\n",
        "            ids=batch_ids,\n",
        "            embeddings=batch_embeddings,\n",
        "            documents=batch_documents,\n",
        "            metadatas=batch_metadatas\n",
        "        )\n",
        "\n",
        "# Insert data\n",
        "start_time = time.time()\n",
        "insert_to_chroma(collection, ids, documents, metadatas, embedding_list, batch_size=100)\n",
        "insert_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n‚úÖ Successfully inserted {len(ids):,} embeddings into ChromaDB\")\n",
        "print(f\"‚è±Ô∏è  Insertion time: {insert_time:.2f} seconds\")\n",
        "print(f\"üöÄ Speed: {len(ids)/insert_time:.1f} insertions/second\")\n",
        "\n",
        "# Verify insertion\n",
        "collection_count = collection.count()\n",
        "print(f\"üìä Collection count: {collection_count:,}\")\n",
        "\n",
        "if collection_count != len(ids):\n",
        "    print(f\"‚ö†Ô∏è  Warning: Expected {len(ids)} but got {collection_count}\")\n",
        "else:\n",
        "    print(f\"‚úÖ All records inserted successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Configuration and Test Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save configuration\n",
        "config = {\n",
        "    'model_name': model_name,\n",
        "    'chunk_size': chunk_size,\n",
        "    'chunk_overlap': chunk_overlap,\n",
        "    'vector_store_path': vector_store_path,\n",
        "    'collection_name': collection_name,\n",
        "    'embedding_dimension': embedding_model.get_sentence_embedding_dimension(),\n",
        "    'total_chunks': len(chunks),\n",
        "    'total_complaints': len(df),\n",
        "    'creation_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "}\n",
        "\n",
        "config_path = os.path.join(vector_store_path, 'config.json')\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Configuration saved to: {config_path}\")\n",
        "print(f\"üìã Configuration:\")\n",
        "for key, value in config.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test retrieval functionality\n",
        "def test_retrieval(collection, embedding_model, query: str, n_results: int = 5):\n",
        "    \"\"\"Test semantic search functionality\"\"\"\n",
        "    \n",
        "    # Generate query embedding\n",
        "    query_embedding = embedding_model.encode([query], normalize_embeddings=True)[0]\n",
        "    \n",
        "    # Search in ChromaDB\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_embedding.tolist()],\n",
        "        n_results=n_results,\n",
        "        include=['documents', 'metadatas', 'distances']\n",
        "    )\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"=== TESTING RETRIEVAL FUNCTIONALITY ===\")\n",
        "\n",
        "test_queries = [\n",
        "    \"billing issues with credit cards\",\n",
        "    \"unauthorized transactions and fraud\",\n",
        "    \"customer service problems\",\n",
        "    \"payment failures and delays\",\n",
        "    \"account access issues\"\n",
        "]\n",
        "\n",
        "for i, query in enumerate(test_queries, 1):\n",
        "    print(f\"\\nüîç Test Query {i}: '{query}'\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    results = test_retrieval(collection, embedding_model, query, n_results=3)\n",
        "    search_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"‚è±Ô∏è  Search time: {search_time*1000:.1f}ms\")\n",
        "    \n",
        "    if results['documents'][0]:\n",
        "        print(f\"üìä Found {len(results['documents'][0])} results:\")\n",
        "        \n",
        "        for j, (doc, metadata, distance) in enumerate(zip(\n",
        "            results['documents'][0], \n",
        "            results['metadatas'][0], \n",
        "            results['distances'][0]\n",
        "        )):\n",
        "            similarity = 1 - distance  # Convert distance to similarity\n",
        "            print(f\"\\n  Result {j+1}:\")\n",
        "            print(f\"    Product: {metadata['product']}\")\n",
        "            print(f\"    Issue: {metadata['issue']}\")\n",
        "            print(f\"    Similarity: {similarity:.3f}\")\n",
        "            print(f\"    Text: {doc[:150]}{'...' if len(doc) > 150 else ''}\")\n",
        "    else:\n",
        "        print(\"‚ùå No results found\")\n",
        "\n",
        "print(f\"\\n‚úÖ Retrieval testing completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Vector Store Statistics and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get comprehensive vector store statistics\n",
        "def get_vector_store_stats(collection):\n",
        "    \"\"\"Get comprehensive statistics about the vector store\"\"\"\n",
        "    \n",
        "    # Get total count\n",
        "    total_count = collection.count()\n",
        "    \n",
        "    # Get sample of metadata for analysis\n",
        "    sample_size = min(1000, total_count)\n",
        "    sample_results = collection.get(\n",
        "        limit=sample_size,\n",
        "        include=['metadatas']\n",
        "    )\n",
        "    \n",
        "    if sample_results['metadatas']:\n",
        "        # Analyze metadata\n",
        "        metadata_df = pd.DataFrame(sample_results['metadatas'])\n",
        "        \n",
        "        stats = {\n",
        "            'total_chunks': total_count,\n",
        "            'sample_size': sample_size,\n",
        "            'product_distribution': metadata_df['product'].value_counts().to_dict(),\n",
        "            'issue_distribution': metadata_df['issue'].value_counts().head(10).to_dict(),\n",
        "            'state_distribution': metadata_df['state'].value_counts().head(10).to_dict(),\n",
        "            'avg_chunk_length': metadata_df['chunk_length'].astype(int).mean(),\n",
        "            'avg_original_length': metadata_df['original_length'].astype(int).mean(),\n",
        "            'unique_complaints': metadata_df['complaint_id'].nunique(),\n",
        "            'avg_chunks_per_complaint': total_count / metadata_df['complaint_id'].nunique()\n",
        "        }\n",
        "        \n",
        "        return stats\n",
        "    \n",
        "    return {'total_chunks': total_count}\n",
        "\n",
        "# Get statistics\n",
        "print(\"=== VECTOR STORE STATISTICS ===\")\n",
        "stats = get_vector_store_stats(collection)\n",
        "\n",
        "print(f\"üìä Total chunks: {stats['total_chunks']:,}\")\n",
        "print(f\"üìù Unique complaints: {stats.get('unique_complaints', 'Unknown'):,}\")\n",
        "print(f\"üìà Average chunks per complaint: {stats.get('avg_chunks_per_complaint', 0):.1f}\")\n",
        "print(f\"üìè Average chunk length: {stats.get('avg_chunk_length', 0):.0f} characters\")\n",
        "print(f\"üìÑ Average original length: {stats.get('avg_original_length', 0):.0f} characters\")\n",
        "\n",
        "if 'product_distribution' in stats:\n",
        "    print(f\"\\nüè∑Ô∏è  Product distribution:\")\n",
        "    for product, count in stats['product_distribution'].items():\n",
        "        percentage = (count / stats['total_chunks']) * 100\n",
        "        print(f\"  - {product}: {count:,} chunks ({percentage:.1f}%)\")\n",
        "\n",
        "if 'issue_distribution' in stats:\n",
        "    print(f\"\\nüéØ Top issues:\")\n",
        "    for issue, count in list(stats['issue_distribution'].items())[:5]:\n",
        "        print(f\"  - {issue}: {count:,} chunks\")\n",
        "\n",
        "# Save statistics\n",
        "stats_path = os.path.join(vector_store_path, 'statistics.json')\n",
        "with open(stats_path, 'w') as f:\n",
        "    json.dump(stats, f, indent=2, default=str)\n",
        "\n",
        "print(f\"\\n‚úÖ Statistics saved to: {stats_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create final visualization\n",
        "if 'product_distribution' in stats:\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Product distribution in vector store\n",
        "    products = list(stats['product_distribution'].keys())\n",
        "    counts = list(stats['product_distribution'].values())\n",
        "    \n",
        "    ax1.pie(counts, labels=products, autopct='%1.1f%%', startangle=90)\n",
        "    ax1.set_title('Chunk Distribution by Product', fontweight='bold')\n",
        "    \n",
        "    # Top issues\n",
        "    issues = list(stats['issue_distribution'].keys())[:8]\n",
        "    issue_counts = list(stats['issue_distribution'].values())[:8]\n",
        "    \n",
        "    ax2.barh(issues, issue_counts, color='lightcoral')\n",
        "    ax2.set_title('Top Issues in Vector Store', fontweight='bold')\n",
        "    ax2.set_xlabel('Number of Chunks')\n",
        "    \n",
        "    # Chunk length distribution (from original data)\n",
        "    chunk_lengths = [c['chunk_length'] for c in chunks]\n",
        "    ax3.hist(chunk_lengths, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "    ax3.set_title('Chunk Length Distribution', fontweight='bold')\n",
        "    ax3.set_xlabel('Chunk Length (characters)')\n",
        "    ax3.set_ylabel('Frequency')\n",
        "    \n",
        "    # Chunks per complaint distribution\n",
        "    chunks_per_complaint = pd.Series([c['complaint_id'] for c in chunks]).value_counts()\n",
        "    ax4.hist(chunks_per_complaint.values, bins=20, alpha=0.7, color='gold', edgecolor='black')\n",
        "    ax4.set_title('Chunks per Complaint Distribution', fontweight='bold')\n",
        "    ax4.set_xlabel('Number of Chunks')\n",
        "    ax4.set_ylabel('Number of Complaints')\n",
        "    \n",
        "    plt.suptitle('Vector Store Analysis Summary', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ TASK 2 COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"‚úÖ Created {stats['total_chunks']:,} text chunks\")\n",
        "print(f\"‚úÖ Generated {stats['total_chunks']:,} embeddings using {model_name}\")\n",
        "print(f\"‚úÖ Stored embeddings in ChromaDB vector store\")\n",
        "print(f\"‚úÖ Tested retrieval functionality successfully\")\n",
        "print(f\"üìÅ Vector store location: {vector_store_path}\")\n",
        "print(f\"‚öôÔ∏è  Configuration saved with chunking and embedding parameters\")\n",
        "print(\"\\nüöÄ Ready for Task 3: Building RAG Core Logic and Evaluation\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
