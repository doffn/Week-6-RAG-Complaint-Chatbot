{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 3: Building the RAG Core Logic and Evaluation\n",
        "\n",
        "## Objective\n",
        "To build the retrieval and generation pipeline and evaluate its effectiveness.\n",
        "\n",
        "## Steps:\n",
        "1. Implement retriever function\n",
        "2. Design robust prompt template\n",
        "3. Implement generator with LLM\n",
        "4. Create comprehensive evaluation framework\n",
        "5. Run qualitative evaluation with test questions\n",
        "6. Analyze results and provide recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from typing import List, Dict, Any, Optional\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ… Libraries imported successfully\")\n",
        "print(f\"ðŸ”§ PyTorch version: {torch.__version__}\")\n",
        "print(f\"ðŸ”§ CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Vector Store and Initialize Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration and vector store\n",
        "vector_store_path = '../vector_store'\n",
        "config_path = os.path.join(vector_store_path, 'config.json')\n",
        "\n",
        "if not os.path.exists(config_path):\n",
        "    print(\"âŒ Vector store not found. Please run Task 2 first.\")\n",
        "    raise FileNotFoundError(\"Run notebook 02_embedding_creation.ipynb first\")\n",
        "\n",
        "# Load configuration\n",
        "with open(config_path, 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "print(\"=== LOADING VECTOR STORE ===\")\n",
        "print(f\"ðŸ“‹ Configuration:\")\n",
        "for key, value in config.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Initialize embedding model\n",
        "print(f\"\\nðŸ”§ Loading embedding model: {config['model_name']}\")\n",
        "embedding_model = SentenceTransformer(config['model_name'])\n",
        "print(f\"âœ… Embedding model loaded\")\n",
        "\n",
        "# Initialize ChromaDB client\n",
        "print(f\"\\nðŸ”§ Connecting to ChromaDB...\")\n",
        "chroma_client = chromadb.PersistentClient(path=vector_store_path)\n",
        "collection = chroma_client.get_collection(name=config['collection_name'])\n",
        "print(f\"âœ… Connected to collection: {config['collection_name']}\")\n",
        "print(f\"ðŸ“Š Collection count: {collection.count():,} chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Implement Retriever Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ComplaintRetriever:\n",
        "    \"\"\"Retriever for complaint embeddings\"\"\"\n",
        "    \n",
        "    def __init__(self, collection, embedding_model):\n",
        "        self.collection = collection\n",
        "        self.embedding_model = embedding_model\n",
        "    \n",
        "    def retrieve(self, query: str, n_results: int = 5, \n",
        "                product_filter: Optional[str] = None) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Retrieve relevant complaint chunks for a given query\"\"\"\n",
        "        \n",
        "        try:\n",
        "            # Generate query embedding\n",
        "            query_embedding = self.embedding_model.encode(\n",
        "                [query], \n",
        "                normalize_embeddings=True,\n",
        "                convert_to_numpy=True\n",
        "            )[0]\n",
        "            \n",
        "            # Prepare filter\n",
        "            where_clause = None\n",
        "            if product_filter and product_filter != \"All Products\":\n",
        "                where_clause = {\"product\": product_filter}\n",
        "            \n",
        "            # Search vector store\n",
        "            results = self.collection.query(\n",
        "                query_embeddings=[query_embedding.tolist()],\n",
        "                n_results=n_results,\n",
        "                where=where_clause,\n",
        "                include=['documents', 'metadatas', 'distances']\n",
        "            )\n",
        "            \n",
        "            # Format results\n",
        "            retrieved_chunks = []\n",
        "            for i in range(len(results['ids'][0])):\n",
        "                chunk = {\n",
        "                    'id': results['ids'][0][i],\n",
        "                    'text': results['documents'][0][i],\n",
        "                    'metadata': results['metadatas'][0][i],\n",
        "                    'similarity_score': 1 - results['distances'][0][i],  # Convert distance to similarity\n",
        "                    'distance': results['distances'][0][i]\n",
        "                }\n",
        "                retrieved_chunks.append(chunk)\n",
        "            \n",
        "            return retrieved_chunks\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error retrieving chunks: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def get_retrieval_stats(self, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        \"\"\"Get statistics about retrieved chunks\"\"\"\n",
        "        if not chunks:\n",
        "            return {}\n",
        "        \n",
        "        similarities = [chunk['similarity_score'] for chunk in chunks]\n",
        "        products = [chunk['metadata']['product'] for chunk in chunks]\n",
        "        issues = [chunk['metadata']['issue'] for chunk in chunks]\n",
        "        \n",
        "        return {\n",
        "            'num_chunks': len(chunks),\n",
        "            'avg_similarity': np.mean(similarities),\n",
        "            'min_similarity': np.min(similarities),\n",
        "            'max_similarity': np.max(similarities),\n",
        "            'products': list(set(products)),\n",
        "            'issues': list(set(issues)),\n",
        "            'top_similarity': similarities[0] if similarities else 0\n",
        "        }\n",
        "\n",
        "# Initialize retriever\n",
        "retriever = ComplaintRetriever(collection, embedding_model)\n",
        "print(\"âœ… Retriever initialized\")\n",
        "\n",
        "# Test retrieval\n",
        "test_query = \"billing issues with credit cards\"\n",
        "print(f\"\\nðŸ” Testing retrieval with query: '{test_query}'\")\n",
        "\n",
        "start_time = time.time()\n",
        "test_results = retriever.retrieve(test_query, n_results=3)\n",
        "retrieval_time = time.time() - start_time\n",
        "\n",
        "print(f\"â±ï¸  Retrieval time: {retrieval_time*1000:.1f}ms\")\n",
        "print(f\"ðŸ“Š Retrieved {len(test_results)} chunks\")\n",
        "\n",
        "if test_results:\n",
        "    stats = retriever.get_retrieval_stats(test_results)\n",
        "    print(f\"ðŸ“ˆ Average similarity: {stats['avg_similarity']:.3f}\")\n",
        "    print(f\"ðŸ·ï¸  Products found: {stats['products']}\")\n",
        "    \n",
        "    print(f\"\\nðŸ“ Sample results:\")\n",
        "    for i, chunk in enumerate(test_results[:2]):\n",
        "        print(f\"  {i+1}. Similarity: {chunk['similarity_score']:.3f} | Product: {chunk['metadata']['product']}\")\n",
        "        print(f\"     Text: {chunk['text'][:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Design Prompt Template\n",
        "\n",
        "### Prompt Engineering Strategy:\n",
        "- **Clear Role Definition**: Establish the AI as a financial analyst assistant\n",
        "- **Context Grounding**: Explicitly instruct to use only provided context\n",
        "- **Structured Output**: Guide the format of responses\n",
        "- **Fallback Handling**: Handle cases with insufficient information\n",
        "- **Evidence-Based**: Encourage citing specific examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PromptTemplate:\n",
        "    \"\"\"Prompt template for RAG system\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.template = \"\"\"You are a financial analyst assistant for CrediTrust Financial. Your task is to analyze customer complaints and provide helpful insights to product managers, support teams, and compliance officers.\n",
        "\n",
        "Based on the following customer complaint excerpts, please answer the user's question. Use only the information provided in the context below. If the context doesn't contain enough information to answer the question completely, state that clearly.\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "- Provide a clear, concise answer based solely on the context above\n",
        "- Highlight key patterns, trends, or recurring themes if relevant\n",
        "- If specific numbers, frequencies, or statistics can be inferred, include them\n",
        "- Mention the financial products involved when relevant\n",
        "- If the context is insufficient, say \"Based on the available complaints, I don't have enough information to fully answer this question.\"\n",
        "- Keep your response focused and actionable for business stakeholders\n",
        "\n",
        "ANSWER:\"\"\"\n",
        "    \n",
        "    def create_context(self, chunks: List[Dict[str, Any]], max_length: int = 2000) -> str:\n",
        "        \"\"\"Create context string from retrieved chunks\"\"\"\n",
        "        context_parts = []\n",
        "        current_length = 0\n",
        "        \n",
        "        for i, chunk in enumerate(chunks):\n",
        "            # Format chunk with metadata\n",
        "            chunk_text = f\"[Source {i+1} - {chunk['metadata']['product']} - {chunk['metadata']['issue']}]: {chunk['text']}\"\n",
        "            \n",
        "            if current_length + len(chunk_text) > max_length:\n",
        "                break\n",
        "            \n",
        "            context_parts.append(chunk_text)\n",
        "            current_length += len(chunk_text)\n",
        "        \n",
        "        return \"\\n\\n\".join(context_parts)\n",
        "    \n",
        "    def create_prompt(self, query: str, chunks: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"Create complete prompt from query and retrieved chunks\"\"\"\n",
        "        context = self.create_context(chunks)\n",
        "        \n",
        "        return self.template.format(\n",
        "            context=context,\n",
        "            question=query\n",
        "        )\n",
        "    \n",
        "    def get_context_stats(self, context: str) -> Dict[str, Any]:\n",
        "        \"\"\"Get statistics about the context\"\"\"\n",
        "        return {\n",
        "            'length': len(context),\n",
        "            'word_count': len(context.split()),\n",
        "            'source_count': context.count('[Source'),\n",
        "            'products_mentioned': len(set([\n",
        "                line.split(' - ')[1] for line in context.split('\\n') \n",
        "                if line.startswith('[Source') and ' - ' in line\n",
        "            ]))\n",
        "        }\n",
        "\n",
        "# Initialize prompt template\n",
        "prompt_template = PromptTemplate()\n",
        "print(\"âœ… Prompt template initialized\")\n",
        "\n",
        "# Test prompt creation\n",
        "test_prompt = prompt_template.create_prompt(test_query, test_results)\n",
        "context_stats = prompt_template.get_context_stats(prompt_template.create_context(test_results))\n",
        "\n",
        "print(f\"\\nðŸ“ Test prompt statistics:\")\n",
        "print(f\"  Total length: {len(test_prompt):,} characters\")\n",
        "print(f\"  Context length: {context_stats['length']:,} characters\")\n",
        "print(f\"  Context word count: {context_stats['word_count']:,} words\")\n",
        "print(f\"  Number of sources: {context_stats['source_count']}\")\n",
        "print(f\"  Products in context: {context_stats['products_mentioned']}\")\n",
        "\n",
        "print(f\"\\nðŸ“„ Sample prompt (first 500 chars):\")\n",
        "print(test_prompt[:500] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Implement Generator with LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ComplaintGenerator:\n",
        "    \"\"\"Generator for complaint analysis responses\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str = \"microsoft/DialoGPT-small\"):\n",
        "        self.model_name = model_name\n",
        "        self.llm_pipeline = None\n",
        "        self.max_response_length = 500\n",
        "        \n",
        "        self._initialize_llm()\n",
        "    \n",
        "    def _initialize_llm(self):\n",
        "        \"\"\"Initialize the language model\"\"\"\n",
        "        try:\n",
        "            print(f\"ðŸ”§ Loading LLM: {self.model_name}\")\n",
        "            device = 0 if torch.cuda.is_available() else -1\n",
        "            \n",
        "            self.llm_pipeline = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=self.model_name,\n",
        "                device=device,\n",
        "                max_length=512,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                pad_token_id=50256,\n",
        "                truncation=True\n",
        "            )\n",
        "            print(f\"âœ… LLM loaded successfully\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸  Could not load advanced LLM: {e}\")\n",
        "            print(f\"ðŸ”„ Using fallback response generation\")\n",
        "            self.llm_pipeline = None\n",
        "    \n",
        "    def generate_response(self, prompt: str) -> str:\n",
        "        \"\"\"Generate response using LLM or fallback method\"\"\"\n",
        "        if self.llm_pipeline is None:\n",
        "            return self._generate_fallback_response(prompt)\n",
        "        \n",
        "        try:\n",
        "            # Generate response using the LLM\n",
        "            response = self.llm_pipeline(\n",
        "                prompt,\n",
        "                max_length=len(prompt) + self.max_response_length,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=50256\n",
        "            )\n",
        "            \n",
        "            # Extract the generated text\n",
        "            generated_text = response[0]['generated_text']\n",
        "            \n",
        "            # Extract only the answer part\n",
        "            if \"ANSWER:\" in generated_text:\n",
        "                answer = generated_text.split(\"ANSWER:\")[-1].strip()\n",
        "            else:\n",
        "                answer = generated_text[len(prompt):].strip()\n",
        "            \n",
        "            return answer if answer else \"I apologize, but I couldn't generate a proper response based on the available information.\"\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error generating response: {e}\")\n",
        "            return self._generate_fallback_response(prompt)\n",
        "    \n",
        "    def _generate_fallback_response(self, prompt: str) -> str:\n",
        "        \"\"\"Generate rule-based response when LLM is not available\"\"\"\n",
        "        # Extract context and question from prompt\n",
        "        if \"CONTEXT:\" in prompt and \"QUESTION:\" in prompt:\n",
        "            context_start = prompt.find(\"CONTEXT:\") + len(\"CONTEXT:\")\n",
        "            question_start = prompt.find(\"QUESTION:\") + len(\"QUESTION:\")\n",
        "            \n",
        "            context = prompt[context_start:prompt.find(\"QUESTION:\")].strip()\n",
        "            question = prompt[question_start:prompt.find(\"INSTRUCTIONS:\")].strip()\n",
        "            \n",
        "            # Simple analysis based on context\n",
        "            if context:\n",
        "                # Count sources\n",
        "                source_count = context.count(\"[Source\")\n",
        "                \n",
        "                # Extract products mentioned\n",
        "                products = []\n",
        "                for product in [\"Credit card\", \"Personal loan\", \"BNPL\", \"Savings account\", \"Money transfers\"]:\n",
        "                    if product.lower() in context.lower():\n",
        "                        products.append(product)\n",
        "                \n",
        "                response = f\"Based on {source_count} relevant complaint(s)\"\n",
        "                if products:\n",
        "                    response += f\" related to {', '.join(products)}\"\n",
        "                response += f\", here are the key insights:\\n\\n\"\n",
        "                \n",
        "                # Extract key themes\n",
        "                themes = []\n",
        "                if \"billing\" in context.lower() or \"charge\" in context.lower():\n",
        "                    themes.append(\"â€¢ Billing and charging issues are prominent concerns\")\n",
        "                if \"customer service\" in context.lower() or \"support\" in context.lower():\n",
        "                    themes.append(\"â€¢ Customer service quality is a recurring theme\")\n",
        "                if \"fraud\" in context.lower() or \"unauthorized\" in context.lower():\n",
        "                    themes.append(\"â€¢ Fraud and unauthorized transactions are reported\")\n",
        "                if \"payment\" in context.lower():\n",
        "                    themes.append(\"â€¢ Payment-related issues are frequently mentioned\")\n",
        "                if \"access\" in context.lower() or \"login\" in context.lower():\n",
        "                    themes.append(\"â€¢ Account access problems are noted\")\n",
        "                if \"fee\" in context.lower():\n",
        "                    themes.append(\"â€¢ Fee-related complaints are present\")\n",
        "                \n",
        "                if themes:\n",
        "                    response += \"\\n\".join(themes) + \"\\n\\n\"\n",
        "                \n",
        "                response += \"The complaints show various customer concerns that may require attention from the relevant product teams.\"\n",
        "                \n",
        "                return response\n",
        "            else:\n",
        "                return \"I don't have enough relevant complaint information to answer this question.\"\n",
        "        \n",
        "        return \"I apologize, but I couldn't process your question properly.\"\n",
        "\n",
        "# Initialize generator\n",
        "generator = ComplaintGenerator()\n",
        "print(\"âœ… Generator initialized\")\n",
        "\n",
        "# Test generation\n",
        "print(f\"\\nðŸ¤– Testing response generation...\")\n",
        "start_time = time.time()\n",
        "test_response = generator.generate_response(test_prompt)\n",
        "generation_time = time.time() - start_time\n",
        "\n",
        "print(f\"â±ï¸  Generation time: {generation_time:.2f} seconds\")\n",
        "print(f\"ðŸ“ Response length: {len(test_response)} characters\")\n",
        "print(f\"\\nðŸ¤– Generated response:\")\n",
        "print(test_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Complete RAG Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RAGPipeline:\n",
        "    \"\"\"Complete RAG pipeline for complaint analysis\"\"\"\n",
        "    \n",
        "    def __init__(self, retriever, prompt_template, generator):\n",
        "        self.retriever = retriever\n",
        "        self.prompt_template = prompt_template\n",
        "        self.generator = generator\n",
        "    \n",
        "    def query(self, question: str, product_filter: Optional[str] = None, \n",
        "              n_results: int = 5) -> Dict[str, Any]:\n",
        "        \"\"\"Process a complete RAG query\"\"\"\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            # Step 1: Retrieve relevant chunks\n",
        "            retrieval_start = time.time()\n",
        "            retrieved_chunks = self.retriever.retrieve(\n",
        "                question, n_results=n_results, product_filter=product_filter\n",
        "            )\n",
        "            retrieval_time = time.time() - retrieval_start\n",
        "            \n",
        "            if not retrieved_chunks:\n",
        "                return {\n",
        "                    'question': question,\n",
        "                    'answer': \"I couldn't find any relevant complaint information to answer your question.\",\n",
        "                    'sources': [],\n",
        "                    'context_used': \"\",\n",
        "                    'product_filter': product_filter,\n",
        "                    'retrieval_time': retrieval_time,\n",
        "                    'generation_time': 0,\n",
        "                    'total_time': time.time() - start_time\n",
        "                }\n",
        "            \n",
        "            # Step 2: Create prompt\n",
        "            prompt = self.prompt_template.create_prompt(question, retrieved_chunks)\n",
        "            \n",
        "            # Step 3: Generate response\n",
        "            generation_start = time.time()\n",
        "            answer = self.generator.generate_response(prompt)\n",
        "            generation_time = time.time() - generation_start\n",
        "            \n",
        "            # Step 4: Format sources for display\n",
        "            sources = []\n",
        "            for i, chunk in enumerate(retrieved_chunks):\n",
        "                source = {\n",
        "                    'index': i + 1,\n",
        "                    'text': chunk['text'][:200] + \"...\" if len(chunk['text']) > 200 else chunk['text'],\n",
        "                    'full_text': chunk['text'],\n",
        "                    'product': chunk['metadata']['product'],\n",
        "                    'issue': chunk['metadata']['issue'],\n",
        "                    'similarity_score': round(chunk['similarity_score'], 3),\n",
        "                    'company': chunk['metadata'].get('company', 'Unknown'),\n",
        "                    'state': chunk['metadata'].get('state', 'Unknown')\n",
        "                }\n",
        "                sources.append(source)\n",
        "            \n",
        "            total_time = time.time() - start_time\n",
        "            \n",
        "            return {\n",
        "                'question': question,\n",
        "                'answer': answer,\n",
        "                'sources': sources,\n",
        "                'context_used': self.prompt_template.create_context(retrieved_chunks),\n",
        "                'product_filter': product_filter,\n",
        "                'num_sources': len(sources),\n",
        "                'retrieval_time': retrieval_time,\n",
        "                'generation_time': generation_time,\n",
        "                'total_time': total_time,\n",
        "                'retrieval_stats': self.retriever.get_retrieval_stats(retrieved_chunks)\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'question': question,\n",
        "                'answer': f\"I encountered an error while processing your question: {str(e)}\",\n",
        "                'sources': [],\n",
        "                'context_used': \"\",\n",
        "                'product_filter': product_filter,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "# Initialize complete RAG pipeline\n",
        "rag_pipeline = RAGPipeline(retriever, prompt_template, generator)\n",
        "print(\"âœ… Complete RAG pipeline initialized\")\n",
        "\n",
        "# Test complete pipeline\n",
        "print(f\"\\nðŸ”„ Testing complete RAG pipeline...\")\n",
        "test_result = rag_pipeline.query(\"What are the main issues customers face with credit cards?\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Pipeline Performance:\")\n",
        "print(f\"  Retrieval time: {test_result.get('retrieval_time', 0)*1000:.1f}ms\")\n",
        "print(f\"  Generation time: {test_result.get('generation_time', 0):.2f}s\")\n",
        "print(f\"  Total time: {test_result.get('total_time', 0):.2f}s\")\n",
        "print(f\"  Sources found: {test_result.get('num_sources', 0)}\")\n",
        "\n",
        "print(f\"\\nðŸ’¬ Generated Answer:\")\n",
        "print(test_result['answer'])\n",
        "\n",
        "if test_result['sources']:\n",
        "    print(f\"\\nðŸ“š Top Sources:\")\n",
        "    for source in test_result['sources'][:2]:\n",
        "        print(f\"  {source['index']}. {source['product']} - Similarity: {source['similarity_score']}\")\n",
        "        print(f\"     {source['text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Comprehensive Evaluation Framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define comprehensive test questions\n",
        "test_questions = [\n",
        "    {\n",
        "        'question': 'What are the main issues customers face with credit cards?',\n",
        "        'expected_themes': ['billing', 'fees', 'fraud', 'unauthorized charges'],\n",
        "        'product_filter': 'Credit card',\n",
        "        'category': 'product_specific'\n",
        "    },\n",
        "    {\n",
        "        'question': 'Why are people unhappy with BNPL services?',\n",
        "        'expected_themes': ['payment issues', 'terms', 'late fees'],\n",
        "        'product_filter': 'Buy Now, Pay Later (BNPL)',\n",
        "        'category': 'product_specific'\n",
        "    },\n",
        "    {\n",
        "        'question': 'What problems do customers report with personal loans?',\n",
        "        'expected_themes': ['approval', 'terms', 'interest rates'],\n",
        "        'product_filter': 'Personal loan',\n",
        "        'category': 'product_specific'\n",
        "    },\n",
        "    {\n",
        "        'question': 'What are common complaints about savings accounts?',\n",
        "        'expected_themes': ['fees', 'access', 'interest'],\n",
        "        'product_filter': 'Savings account',\n",
        "        'category': 'product_specific'\n",
        "    },\n",
        "    {\n",
        "        'question': 'What issues do customers have with money transfers?',\n",
        "        'expected_themes': ['delays', 'fees', 'failed transfers'],\n",
        "        'product_filter': 'Money transfers',\n",
        "        'category': 'product_specific'\n",
        "    },\n",
        "    {\n",
        "        'question': 'Which financial product has the most fraud-related complaints?',\n",
        "        'expected_themes': ['fraud comparison', 'unauthorized transactions'],\n",
        "        'product_filter': None,\n",
        "        'category': 'comparative'\n",
        "    },\n",
        "    {\n",
        "        'question': 'What are customers saying about customer service quality?',\n",
        "        'expected_themes': ['service quality', 'response time', 'helpfulness'],\n",
        "        'product_filter': None,\n",
        "        'category': 'cross_product'\n",
        "    },\n",
        "    {\n",
        "        'question': 'Are there any patterns in billing disputes across products?',\n",
        "        'expected_themes': ['billing issues', 'unauthorized charges', 'fee disputes'],\n",
        "        'product_filter': None,\n",
        "        'category': 'pattern_analysis'\n",
        "    },\n",
        "    {\n",
        "        'question': 'What are the most frequent complaint types?',\n",
        "        'expected_themes': ['complaint frequency', 'issue distribution'],\n",
        "        'product_filter': None,\n",
        "        'category': 'analytical'\n",
        "    },\n",
        "    {\n",
        "        'question': 'How do customers describe unauthorized transactions?',\n",
        "        'expected_themes': ['fraud descriptions', 'unauthorized activity'],\n",
        "        'product_filter': None,\n",
        "        'category': 'descriptive'\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"ðŸ“‹ Defined {len(test_questions)} test questions across {len(set(q['category'] for q in test_questions))} categories\")\n",
        "print(f\"ðŸ“Š Categories: {', '.join(set(q['category'] for q in test_questions))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run comprehensive evaluation\n",
        "def run_evaluation(rag_pipeline, test_questions):\n",
        "    \"\"\"Run comprehensive evaluation of the RAG system\"\"\"\n",
        "    \n",
        "    print(\"=== RUNNING COMPREHENSIVE EVALUATION ===\")\n",
        "    print(f\"Testing {len(test_questions)} questions...\\n\")\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for i, test_case in enumerate(tqdm(test_questions, desc=\"Evaluating questions\")):\n",
        "        question = test_case['question']\n",
        "        expected_themes = test_case['expected_themes']\n",
        "        product_filter = test_case['product_filter']\n",
        "        category = test_case['category']\n",
        "        \n",
        "        # Get RAG response\n",
        "        result = rag_pipeline.query(question, product_filter=product_filter)\n",
        "        \n",
        "        # Calculate evaluation metrics\n",
        "        has_sources = len(result['sources']) > 0\n",
        "        answer_length = len(result['answer'])\n",
        "        \n",
        "        # Check if answer mentions relevant products\n",
        "        product_keywords = ['credit card', 'loan', 'bnpl', 'savings', 'transfer', 'money']\n",
        "        mentions_product = any(keyword in result['answer'].lower() for keyword in product_keywords)\n",
        "        \n",
        "        # Check for expected themes\n",
        "        themes_found = sum(1 for theme in expected_themes \n",
        "                          if theme.lower() in result['answer'].lower())\n",
        "        theme_coverage = themes_found / len(expected_themes) if expected_themes else 0\n",
        "        \n",
        "        # Calculate quality score\n",
        "        quality_score = calculate_quality_score(result, has_sources, mentions_product, theme_coverage)\n",
        "        \n",
        "        evaluation_result = {\n",
        "            'question_id': i + 1,\n",
        "            'question': question,\n",
        "            'category': category,\n",
        "            'product_filter': product_filter or 'All',\n",
        "            'answer': result['answer'],\n",
        "            'answer_preview': result['answer'][:150] + \"...\" if len(result['answer']) > 150 else result['answer'],\n",
        "            'num_sources': len(result['sources']),\n",
        "            'has_sources': has_sources,\n",
        "            'answer_length': answer_length,\n",
        "            'mentions_product': mentions_product,\n",
        "            'expected_themes': expected_themes,\n",
        "            'themes_found': themes_found,\n",
        "            'theme_coverage': theme_coverage,\n",
        "            'quality_score': quality_score,\n",
        "            'top_similarity': result['sources'][0]['similarity_score'] if result['sources'] else 0,\n",
        "            'retrieval_time': result.get('retrieval_time', 0),\n",
        "            'generation_time': result.get('generation_time', 0),\n",
        "            'total_time': result.get('total_time', 0),\n",
        "            'sources': result['sources']\n",
        "        }\n",
        "        \n",
        "        results.append(evaluation_result)\n",
        "    \n",
        "    return results\n",
        "\n",
        "def calculate_quality_score(result, has_sources, mentions_product, theme_coverage):\n",
        "    \"\"\"Calculate quality score from 1-5\"\"\"\n",
        "    score = 1\n",
        "    \n",
        "    # Has sources\n",
        "    if has_sources:\n",
        "        score += 1\n",
        "    \n",
        "    # Good similarity\n",
        "    if result['sources'] and result['sources'][0]['similarity_score'] > 0.7:\n",
        "        score += 1\n",
        "    \n",
        "    # Reasonable answer length\n",
        "    if 50 <= len(result['answer']) <= 400:\n",
        "        score += 1\n",
        "    \n",
        "    # Theme coverage or product mention\n",
        "    if theme_coverage > 0.3 or mentions_product:\n",
        "        score += 1\n",
        "    \n",
        "    return min(score, 5)\n",
        "\n",
        "# Run evaluation\n",
        "evaluation_results = run_evaluation(rag_pipeline, test_questions)\n",
        "results_df = pd.DataFrame(evaluation_results)\n",
        "\n",
        "print(f\"\\nâœ… Evaluation completed!\")\n",
        "print(f\"ðŸ“Š Processed {len(evaluation_results)} questions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluation Analysis and Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate comprehensive metrics\n",
        "def calculate_evaluation_metrics(results_df):\n",
        "    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
        "    \n",
        "    total_questions = len(results_df)\n",
        "    \n",
        "    metrics = {\n",
        "        'total_questions': total_questions,\n",
        "        'questions_with_sources': results_df['has_sources'].sum(),\n",
        "        'source_retrieval_rate': results_df['has_sources'].mean() * 100,\n",
        "        'avg_sources_per_question': results_df['num_sources'].mean(),\n",
        "        'avg_answer_length': results_df['answer_length'].mean(),\n",
        "        'questions_mentioning_products': results_df['mentions_product'].sum(),\n",
        "        'product_mention_rate': results_df['mentions_product'].mean() * 100,\n",
        "        'avg_top_similarity': results_df['top_similarity'].mean(),\n",
        "        'avg_theme_coverage': results_df['theme_coverage'].mean() * 100,\n",
        "        'avg_quality_score': results_df['quality_score'].mean(),\n",
        "        'high_quality_responses': (results_df['quality_score'] >= 4).sum(),\n",
        "        'avg_retrieval_time': results_df['retrieval_time'].mean() * 1000,  # ms\n",
        "        'avg_generation_time': results_df['generation_time'].mean(),\n",
        "        'avg_total_time': results_df['total_time'].mean()\n",
        "    }\n",
        "    \n",
        "    # Category-wise analysis\n",
        "    category_metrics = {}\n",
        "    for category in results_df['category'].unique():\n",
        "        cat_df = results_df[results_df['category'] == category]\n",
        "        category_metrics[category] = {\n",
        "            'count': len(cat_df),\n",
        "            'avg_quality': cat_df['quality_score'].mean(),\n",
        "            'source_rate': cat_df['has_sources'].mean() * 100,\n",
        "            'avg_similarity': cat_df['top_similarity'].mean()\n",
        "        }\n",
        "    \n",
        "    metrics['category_analysis'] = category_metrics\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# Calculate metrics\n",
        "metrics = calculate_evaluation_metrics(results_df)\n",
        "\n",
        "print(\"=== EVALUATION RESULTS ===\")\n",
        "print(f\"\\nðŸ“Š Overall Performance:\")\n",
        "print(f\"  Total questions: {metrics['total_questions']}\")\n",
        "print(f\"  Source retrieval rate: {metrics['source_retrieval_rate']:.1f}%\")\n",
        "print(f\"  Average sources per question: {metrics['avg_sources_per_question']:.1f}\")\n",
        "print(f\"  Product mention rate: {metrics['product_mention_rate']:.1f}%\")\n",
        "print(f\"  Average similarity score: {metrics['avg_top_similarity']:.3f}\")\n",
        "print(f\"  Average theme coverage: {metrics['avg_theme_coverage']:.1f}%\")\n",
        "print(f\"  Average quality score: {metrics['avg_quality_score']:.1f}/5\")\n",
        "print(f\"  High quality responses (â‰¥4): {metrics['high_quality_responses']}/{metrics['total_questions']}\")\n",
        "\n",
        "print(f\"\\nâ±ï¸  Performance Timing:\")\n",
        "print(f\"  Average retrieval time: {metrics['avg_retrieval_time']:.1f}ms\")\n",
        "print(f\"  Average generation time: {metrics['avg_generation_time']:.2f}s\")\n",
        "print(f\"  Average total time: {metrics['avg_total_time']:.2f}s\")\n",
        "\n",
        "print(f\"\\nðŸ“ˆ Category Analysis:\")\n",
        "for category, cat_metrics in metrics['category_analysis'].items():\n",
        "    print(f\"  {category.replace('_', ' ').title()}:\")\n",
        "    print(f\"    Questions: {cat_metrics['count']}\")\n",
        "    print(f\"    Avg quality: {cat_metrics['avg_quality']:.1f}/5\")\n",
        "    print(f\"    Source rate: {cat_metrics['source_rate']:.1f}%\")\n",
        "    print(f\"    Avg similarity: {cat_metrics['avg_similarity']:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualizations\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Quality score distribution\n",
        "quality_counts = results_df['quality_score'].value_counts().sort_index()\n",
        "ax1.bar(quality_counts.index, quality_counts.values, color='skyblue', alpha=0.7)\n",
        "ax1.set_title('Distribution of Quality Scores', fontweight='bold', fontsize=14)\n",
        "ax1.set_xlabel('Quality Score (1-5)')\n",
        "ax1.set_ylabel('Number of Questions')\n",
        "ax1.set_xticks(range(1, 6))\n",
        "for i, v in enumerate(quality_counts.values):\n",
        "    ax1.text(quality_counts.index[i], v + 0.1, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "# Performance by category\n",
        "category_quality = results_df.groupby('category')['quality_score'].mean().sort_values(ascending=True)\n",
        "ax2.barh(range(len(category_quality)), category_quality.values, color='lightcoral', alpha=0.7)\n",
        "ax2.set_yticks(range(len(category_quality)))\n",
        "ax2.set_yticklabels([cat.replace('_', ' ').title() for cat in category_quality.index])\n",
        "ax2.set_title('Average Quality Score by Category', fontweight='bold', fontsize=14)\n",
        "ax2.set_xlabel('Average Quality Score')\n",
        "ax2.set_xlim(0, 5)\n",
        "for i, v in enumerate(category_quality.values):\n",
        "    ax2.text(v + 0.1, i, f'{v:.1f}', va='center', fontweight='bold')\n",
        "\n",
        "# Similarity vs Quality correlation\n",
        "scatter = ax3.scatter(results_df['top_similarity'], results_df['quality_score'], \n",
        "                     c=results_df['num_sources'], cmap='viridis', alpha=0.7, s=60)\n",
        "ax3.set_title('Similarity Score vs Quality Score', fontweight='bold', fontsize=14)\n",
        "ax3.set_xlabel('Top Similarity Score')\n",
        "ax3.set_ylabel('Quality Score')\n",
        "plt.colorbar(scatter, ax=ax3, label='Number of Sources')\n",
        "\n",
        "# Response time analysis\n",
        "ax4.hist(results_df['total_time'], bins=15, alpha=0.7, color='gold', edgecolor='black')\n",
        "ax4.set_title('Distribution of Response Times', fontweight='bold', fontsize=14)\n",
        "ax4.set_xlabel('Total Time (seconds)')\n",
        "ax4.set_ylabel('Frequency')\n",
        "ax4.axvline(results_df['total_time'].mean(), color='red', linestyle='--', \n",
        "           label=f'Mean: {results_df[\"total_time\"].mean():.2f}s')\n",
        "ax4.legend()\n",
        "\n",
        "plt.suptitle('RAG System Evaluation Analysis', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation analysis\n",
        "print(f\"\\nðŸ”— Correlation Analysis:\")\n",
        "correlations = results_df[['quality_score', 'top_similarity', 'num_sources', 'answer_length', 'theme_coverage']].corr()\n",
        "print(f\"Quality Score correlations:\")\n",
        "quality_corr = correlations['quality_score'].drop('quality_score').sort_values(ascending=False)\n",
        "for metric, corr in quality_corr.items():\n",
        "    print(f\"  {metric}: {corr:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed results table\n",
        "print(\"=== DETAILED EVALUATION RESULTS ===\")\n",
        "print(\"\\nTop performing questions:\")\n",
        "top_questions = results_df.nlargest(3, 'quality_score')[['question', 'category', 'quality_score', 'top_similarity', 'num_sources']]\n",
        "for _, row in top_questions.iterrows():\n",
        "    print(f\"\\nâœ… Q: {row['question'][:60]}...\")\n",
        "    print(f\"   Category: {row['category']} | Quality: {row['quality_score']}/5 | Similarity: {row['top_similarity']:.3f} | Sources: {row['num_sources']}\")\n",
        "\n",
        "print(\"\\nLowest performing questions:\")\n",
        "low_questions = results_df.nsmallest(3, 'quality_score')[['question', 'category', 'quality_score', 'top_similarity', 'num_sources']]\n",
        "for _, row in low_questions.iterrows():\n",
        "    print(f\"\\nâš ï¸  Q: {row['question'][:60]}...\")\n",
        "    print(f\"   Category: {row['category']} | Quality: {row['quality_score']}/5 | Similarity: {row['top_similarity']:.3f} | Sources: {row['num_sources']}\")\n",
        "\n",
        "# Sample responses\n",
        "print(\"\\n=== SAMPLE RESPONSES ===\")\n",
        "sample_result = results_df.iloc[0]\n",
        "print(f\"\\nðŸ“ Question: {sample_result['question']}\")\n",
        "print(f\"ðŸ¤– Answer: {sample_result['answer']}\")\n",
        "print(f\"ðŸ“Š Quality Score: {sample_result['quality_score']}/5\")\n",
        "print(f\"ðŸ“š Sources: {sample_result['num_sources']}\")\n",
        "if sample_result['sources']:\n",
        "    print(f\"ðŸ” Top Source: {sample_result['sources'][0]['product']} - {sample_result['sources'][0]['similarity_score']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Results and Generate Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save evaluation results\n",
        "os.makedirs('../results', exist_ok=True)\n",
        "\n",
        "# Save detailed results\n",
        "results_df.to_csv('../results/rag_evaluation_results.csv', index=False)\n",
        "print(f\"âœ… Detailed results saved to: ../results/rag_evaluation_results.csv\")\n",
        "\n",
        "# Save metrics\n",
        "with open('../results/evaluation_metrics.json', 'w') as f:\n",
        "    json.dump(metrics, f, indent=2, default=str)\n",
        "print(f\"âœ… Metrics saved to: ../results/evaluation_metrics.json\")\n",
        "\n",
        "# Generate analysis report\n",
        "def generate_analysis_report(metrics, results_df):\n",
        "    \"\"\"Generate comprehensive analysis report\"\"\"\n",
        "    \n",
        "    report = {\n",
        "        'strengths': [],\n",
        "        'weaknesses': [],\n",
        "        'recommendations': []\n",
        "    }\n",
        "    \n",
        "    # Analyze strengths\n",
        "    if metrics['source_retrieval_rate'] > 80:\n",
        "        report['strengths'].append(f\"High source retrieval rate ({metrics['source_retrieval_rate']:.1f}%) - system consistently finds relevant information\")\n",
        "    \n",
        "    if metrics['avg_top_similarity'] > 0.7:\n",
        "        report['strengths'].append(f\"High similarity scores ({metrics['avg_top_similarity']:.3f}) indicate good semantic matching\")\n",
        "    \n",
        "    if metrics['product_mention_rate'] > 60:\n",
        "        report['strengths'].append(f\"Good product awareness ({metrics['product_mention_rate']:.1f}%) - answers frequently mention relevant products\")\n",
        "    \n",
        "    if metrics['avg_total_time'] < 3:\n",
        "        report['strengths'].append(f\"Fast response times ({metrics['avg_total_time']:.2f}s average) suitable for interactive use\")\n",
        "    \n",
        "    # Analyze weaknesses\n",
        "    no_source_questions = results_df[results_df['num_sources'] == 0]\n",
        "    if len(no_source_questions) > 0:\n",
        "        report['weaknesses'].append(f\"{len(no_source_questions)} questions returned no sources\")\n",
        "    \n",
        "    short_answers = results_df[results_df['answer_length'] < 50]\n",
        "    if len(short_answers) > 0:\n",
        "        report['weaknesses'].append(f\"{len(short_answers)} questions produced very short answers\")\n",
        "    \n",
        "    low_quality = results_df[results_df['quality_score'] < 3]\n",
        "    if len(low_quality) > 0:\n",
        "        report['weaknesses'].append(f\"{len(low_quality)} questions received low quality scores (<3/5)\")\n",
        "    \n",
        "    if metrics['avg_theme_coverage'] < 40:\n",
        "        report['weaknesses'].append(f\"Low theme coverage ({metrics['avg_theme_coverage']:.1f}%) - answers may miss expected topics\")\n",
        "    \n",
        "    # Generate recommendations\n",
        "    if metrics['avg_top_similarity'] < 0.6:\n",
        "        report['recommendations'].append(\"Consider improving embedding model or chunking strategy for better semantic matching\")\n",
        "    \n",
        "    if metrics['avg_answer_length'] > 300 or results_df['answer_length'].std() > 150:\n",
        "        report['recommendations'].append(\"Standardize answer length for consistency and readability\")\n",
        "    \n",
        "    report['recommendations'].extend([\n",
        "        \"Implement user feedback mechanism to continuously improve response quality\",\n",
        "        \"Add confidence scores to help users assess answer reliability\",\n",
        "        \"Consider fine-tuning the language model on financial complaint data\",\n",
        "        \"Expand the evaluation dataset with more diverse question types\",\n",
        "        \"Monitor system performance in production with real user queries\"\n",
        "    ])\n",
        "    \n",
        "    return report\n",
        "\n",
        "# Generate analysis\n",
        "analysis = generate_analysis_report(metrics, results_df)\n",
        "\n",
        "print(\"\\n=== ANALYSIS SUMMARY ===\")\n",
        "print(\"\\nâœ… Strengths:\")\n",
        "for strength in analysis['strengths']:\n",
        "    print(f\"  â€¢ {strength}\")\n",
        "\n",
        "print(\"\\nâš ï¸ Areas for Improvement:\")\n",
        "for weakness in analysis['weaknesses']:\n",
        "    print(f\"  â€¢ {weakness}\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Recommendations:\")\n",
        "for rec in analysis['recommendations']:\n",
        "    print(f\"  â€¢ {rec}\")\n",
        "\n",
        "# Save analysis\n",
        "with open('../results/evaluation_analysis.json', 'w') as f:\n",
        "    json.dump(analysis, f, indent=2)\n",
        "print(f\"\\nâœ… Analysis saved to: ../results/evaluation_analysis.json\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸŽ‰ TASK 3 COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"âœ… Implemented complete RAG pipeline with retriever, prompt template, and generator\")\n",
        "print(f\"âœ… Evaluated system with {len(test_questions)} comprehensive test questions\")\n",
        "print(f\"âœ… Achieved {metrics['source_retrieval_rate']:.1f}% source retrieval rate\")\n",
        "print(f\"âœ… Average quality score: {metrics['avg_quality_score']:.1f}/5\")\n",
        "print(f\"âœ… Generated detailed analysis and recommendations\")\n",
        "print(f\"ðŸ“ Results saved to: ../results/\")\n",
        "print(\"\\nðŸš€ Ready for Task 4: Creating Interactive Chat Interface\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
